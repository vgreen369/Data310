### Lab 4

# Question 1

Regularization is defined as The minimization of the sum of squared residuals subject to a constraint on the weights (aka coefficients).

# Question 2
The regularization with the square of an L2 distance may improve the results compared to OLS when the number of features is higher than the number of observations.

True, 
# Question 3
The L1 norm always yields shorter distances compared to the Euclidean norm. 

False
# Question 4
Typically, the regularization is achieved by
minimizing the average of the squared residuals plus a penalty function whose input is the vector of coefficients.

# Question 5
A regularization method that facilitates variable selection (estimating some coefficients as zero) is Lasso

# Question 6
![data](lab4_6.PNG)

# Question 7 
![data](7.PNG)

# Question 8
The gradient descent method does not need any hyperparameters.


False, needs a 'b' and an 'm'.

# Question 9 
![steps](9.PNG)
1. import matplotlib
2. create the subplots
3. add the data to the scatterplots
4. add grids and ticks to the plots

# Question 10
Which of the following forms is not linear in the weights?

![equation](10.PNG)


